# TJR101 Group 2 – Airflow ETL Pipeline

This repository contains the Airflow project for scheduling and executing ETL workflows used by **TJR101 Group 2**. The core ETL logic is implemented in a separate project folder and mounted into the Airflow container via Docker Compose.

---

## 📁 Project Structure

```
VM/
├── TJR101_group2_airflow/         # Airflow project folder (scheduling & orchestration)
│   ├── dags/                      # DAG definitions (import modules from ../src/)
│   ├── logs/                      # Logs generated by Airflow tasks
│   ├── plugins/                   # (Optional) Custom plugins for Airflow
│   ├── config/                    # (Optional) Extra configs (e.g. custom providers)
│   ├── .env                       # Environment variables (e.g. DB credentials, PYTHONPATH)
│   ├── Dockerfile                 # Custom Dockerfile to build Airflow image with dependencies
│   ├── docker-compose.yaml        # Docker Compose configuration for Airflow services
│   ├── poetry.lock                # Locked dependency versions for Airflow
│   └── pyproject.toml             # Dependency definitions for Airflow (Poetry)
│   
├── TJR101_group2/                 # ETL source project (main data processing logic)
│   ├── src/                       # All data processing code (ETL modules, utils, etc.)
│   ├── data/                      # Raw data, intermediate files, and error logs
│   ├── poetry.lock                # Locked dependencies for the ETL codebase (Poetry)
│   └── ...
```

---

## 🛠️ Setup Instructions

### Install Docker Compose (for Ubuntu)

Install Docker Engine on Ubuntu
https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository

Install the Docker Compose plugin
https://docs.docker.com/compose/install/linux/#install-using-the-repository


### Step 1: Create or clone the Airflow project directory

```bash
mkdir -p airflow
cd airflow
```

### Step 2: Create required folders and .env file

```bash
mkdir -p ./dags ./logs ./plugins ./config
echo "AIRFLOW_UID=$(id -u)" > .env
echo "PYTHONPATH=/opt/airflow/../src" >> .env  # Optional: to allow DAGs to import external modules
```

### Step 3: Download the official docker-compose.yaml

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.5/docker-compose.yaml'
```

### Step 4: Modify docker-compose.yaml (volume mount paths)

```yaml
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  - ${AIRFLOW_PROJ_DIR:-.}/../src:/opt/airflow/src
  - ${AIRFLOW_PROJ_DIR:-.}/../data:/opt/airflow/data
```

### Step 5: (Optional) Use a custom Dockerfile with Poetry support

In docker-compose.yaml, replace:

```yaml
# image: apache/airflow:2.10.5
build:
  context: .
  dockerfile: Dockerfile
image: airflow-poetry:TJR101_group2
```

Then create or update your Dockerfile with the following content:

```dockerfile
FROM apache/airflow:2.10.5

USER root

COPY pyproject.toml poetry.lock* /opt/project/
COPY src /opt/project/src

WORKDIR /opt/project
RUN poetry config virtualenvs.create false && \
    poetry install --no-interaction --no-ansi

USER airflow
```

### Step 6: Additional docker-compose.yaml tweaks

```yaml
# Replace init command
command: version

# Disable default example DAGs
environment:
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
```

### Step 7: Build and start Airflow

```bash
docker-compose build            # Build image
docker-compose up airflow-init  # Initialize DB and user
docker-compose up -d            # Start Airflow services
```

### Step 8: Access the Airflow UI

```bash
# URL: http://localhost:8080
# Default credentials: airflow / airflow
```
