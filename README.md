# TJR101 Group 2 â€“ Airflow ETL Pipeline

This repository contains the Airflow project for scheduling and executing ETL workflows used by **TJR101 Group 2**. The core ETL logic is implemented in a separate project folder and mounted into the Airflow container via Docker Compose.

---

## ðŸ“ Project Structure

```
TJR101_group2_airflow/         # Airflow project folder (scheduling & orchestration)
â”œâ”€â”€ dags/                      # DAG definitions (import modules from ../src/)
â”œâ”€â”€ logs/                      # Logs generated by Airflow tasks
â”œâ”€â”€ plugins/                   # (Optional) Custom plugins for Airflow
â”œâ”€â”€ config/                    # (Optional) Extra configs (e.g. custom providers)
â”œâ”€â”€ .env                       # Environment variables (e.g. DB credentials, PYTHONPATH)
â”œâ”€â”€ Dockerfile                 # Custom Dockerfile to build Airflow image with dependencies
â”œâ”€â”€ docker-compose.yaml        # Docker Compose configuration for Airflow services
â”œâ”€â”€ poetry.lock                # Locked dependency versions for Airflow
â””â”€â”€ pyproject.toml             # Dependency definitions for Airflow (Poetry)

TJR101_group2/                 # ETL source project (main data processing logic)
â”œâ”€â”€ src/                       # All data processing code (ETL modules, utils, etc.)
â”œâ”€â”€ data/                      # Raw data, intermediate files, and error logs
â””â”€â”€ poetry.lock                # Locked dependencies for the ETL codebase (Poetry)
```

---

## ðŸ› ï¸ Setup Instructions

### Step 1: Create or clone the Airflow project directory

```bash
mkdir -p airflow
cd airflow
```

### Step 2: Create required folders and .env file

```bash
mkdir -p ./dags ./logs ./plugins ./config
echo "AIRFLOW_UID=$(id -u)" > .env
echo "PYTHONPATH=/opt/airflow/../src" >> .env  # Optional: to allow DAGs to import external modules
```

### Step 3: Download the official docker-compose.yaml

```bash
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/3.0.0/docker-compose.yaml'
```

### Step 4: Modify docker-compose.yaml (volume mount paths)

```yaml
volumes:
  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
  - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  - ${AIRFLOW_PROJ_DIR:-.}/../src:/opt/airflow/src
  - ${AIRFLOW_PROJ_DIR:-.}/../data:/opt/airflow/data
```

### Step 5: (Optional) Use a custom Dockerfile with Poetry support

In docker-compose.yaml, replace:

```yaml
# image: apache/airflow:2.10.5
build:
  context: ..
  dockerfile: airflow/Dockerfile
image: airflow-poetry:TJR101_group2
```

Then create Dockerfile:

```dockerfile
FROM apache/airflow:2.10.5

USER root

COPY pyproject.toml poetry.lock* /opt/project/
COPY src /opt/project/src

WORKDIR /opt/project
RUN poetry config virtualenvs.create false && \
    poetry install --no-interaction --no-ansi

USER airflow
```

### Step 6: Additional docker-compose.yaml tweaks

```yaml
# Replace init command
command: version

# Disable default example DAGs
environment:
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
```

### Step 7: Build and start Airflow

```bash
docker-compose build            # Build image
docker-compose up airflow-init  # Initialize DB and user
docker-compose up -d            # Start Airflow services
```

### Step 8: Access the Airflow UI

```bash
# URL: http://localhost:8080
# Default credentials: airflow / airflow
```
